{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bfc3328-15f2-46fc-bfda-b7bf22fa213c",
   "metadata": {},
   "source": [
    "# Earthquake data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e8e2d-0b4e-426d-8562-3954c4e5df8e",
   "metadata": {},
   "source": [
    "## A. Load and Inspect Data\n",
    "This section involves reading the dataset and understanding its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63b505-1547-4f90-9c05-d3c1a6eee4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library\n",
    "import pandas as pd\n",
    "\n",
    "# A. Load and Inspect Data\n",
    "# Step 1: Load data from the CSV file\n",
    "# Replace the file path with the actual location of your earthquake data CSV file\n",
    "file_path = \"path_to_file/earthquake_data.csv\"  # Adjust file path as needed\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Inspect the structure of the dataset\n",
    "# Display information about the columns, data types, and non-null counts\n",
    "print(\"Dataset Information:\")\n",
    "print(data.info())\n",
    "\n",
    "# Step 3: Preview the first few rows of the dataset\n",
    "# Display the first 5 rows to understand the content and format of the data\n",
    "print(\"\\nDataset Preview:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efff07c-742c-4c48-a06c-9aec2171614c",
   "metadata": {},
   "source": [
    "## B. Data Cleaning\n",
    "\n",
    "Steps include handling missing values, filtering relevant rows, and formatting data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5a21ed-499f-4c2f-ba01-d8bd5fb0344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. Data Cleaning\n",
    "# Step 1: Drop rows with missing values in critical columns\n",
    "# Critical columns: 'longitude', 'latitude', 'magnitude', and 'depth'\n",
    "# These columns are essential for earthquake analysis\n",
    "data.dropna(subset=['longitude', 'latitude', 'magnitude', 'depth'], inplace=True)\n",
    "print(\"\\nRows with missing critical values dropped.\")\n",
    "\n",
    "# Step 2: Filter only earthquake events\n",
    "# The dataset might contain other event types; we are only interested in earthquakes\n",
    "# Filter rows where 'eventtype' is equal to 'earthquake'\n",
    "data = data[data['eventtype'] == 'earthquake']\n",
    "print(\"\\nFiltered dataset to include only earthquake events.\")\n",
    "\n",
    "# Step 3: Convert the 'origintime' column to datetime format\n",
    "# Ensure the 'origintime' column is in datetime format for temporal analysis\n",
    "data['origintime'] = pd.to_datetime(data['origintime'], errors='coerce')\n",
    "print(\"\\nConverted 'origintime' column to datetime format.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da639e8-b305-43e8-8405-fac663d76048",
   "metadata": {},
   "source": [
    "## C. Final Dataset Review\n",
    "The cleaned dataset is inspected again to ensure all preprocessing steps have been correctly applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd53e6-fda0-4a0d-af3d-ac197d4726f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final step: Display the cleaned dataset's structure and preview\n",
    "print(\"\\nCleaned Dataset Information:\")\n",
    "print(data.info())\n",
    "print(\"\\nCleaned Dataset Preview:\")\n",
    "print(data.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
